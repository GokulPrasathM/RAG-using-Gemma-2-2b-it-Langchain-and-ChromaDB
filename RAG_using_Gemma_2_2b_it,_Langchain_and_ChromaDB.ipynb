{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 89475,
          "databundleVersionId": 10464219,
          "sourceType": "competition"
        },
        {
          "sourceId": 4737381,
          "sourceType": "datasetVersion",
          "datasetId": 2740486
        },
        {
          "sourceId": 10716,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 8658,
          "modelId": 1445
        },
        {
          "sourceId": 28785,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 8318,
          "modelId": 3301
        }
      ],
      "dockerImageVersionId": 30559,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "RAG using Gemma 2 2b-it, Langchain and ChromaDB",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "VKfR-mmjijN6"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "casml_generative_ai_hackathon_path = kagglehub.competition_download('casml-generative-ai-hackathon')\n",
        "erenakbulut_sentence_transformers_path = kagglehub.dataset_download('erenakbulut/sentence-transformers')\n",
        "microsoft_phi_transformers_2_1_path = kagglehub.model_download('Microsoft/phi/Transformers/2/1')\n",
        "google_gemma_transformers_2b_it_3_path = kagglehub.model_download('google/gemma/Transformers/2b-it/3')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Zj7RbKvjijOA"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21\n",
        "!pip install bitsandbytes==0.41.1 chromadb==0.4.12 datasets accelerate --upgrade huggingface_hub\n",
        "!pip install torchvision==0.4.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install -U sentence-transformers pypdf2 pdfplumber"
      ],
      "metadata": {
        "_uuid": "a108bd51-0e27-4ba4-9325-5484bb98cfb0",
        "_cell_guid": "bf38837a-5e03-4e26-beb2-8be91b9e1b98",
        "trusted": true,
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-12-19T17:41:02.293234Z",
          "iopub.execute_input": "2024-12-19T17:41:02.293528Z",
          "iopub.status.idle": "2024-12-19T17:44:22.634858Z",
          "shell.execute_reply.started": "2024-12-19T17:41:02.293487Z",
          "shell.execute_reply": "2024-12-19T17:44:22.63369Z"
        },
        "id": "TVb1xr8jijOF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from torch import cuda, bfloat16\n",
        "import torch\n",
        "import transformers\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "_uuid": "88063113-4510-44ef-bd50-1dfa7ba51ed9",
        "_cell_guid": "e89ee724-c3ef-4854-a8ee-ec23ff36f9b5",
        "trusted": true,
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-12-20T12:35:14.676981Z",
          "iopub.execute_input": "2024-12-20T12:35:14.677299Z",
          "iopub.status.idle": "2024-12-20T12:35:18.396653Z",
          "shell.execute_reply.started": "2024-12-20T12:35:14.677272Z",
          "shell.execute_reply": "2024-12-20T12:35:18.395262Z"
        },
        "id": "J5CJm3a7ijOH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = '/kaggle/input/gemma/transformers/2b-it/3'"
      ],
      "metadata": {
        "_uuid": "54916abd-4b2b-4c8a-9f05-ffc0da214aef",
        "_cell_guid": "3a2fbe09-fb03-4061-b120-b2b5f03bd6db",
        "trusted": true,
        "_kg_hide-input": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-12-19T17:44:45.427848Z",
          "iopub.execute_input": "2024-12-19T17:44:45.428396Z",
          "iopub.status.idle": "2024-12-19T17:44:45.432392Z",
          "shell.execute_reply.started": "2024-12-19T17:44:45.428367Z",
          "shell.execute_reply": "2024-12-19T17:44:45.431532Z"
        },
        "id": "SU08T7s7ijOI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-19T17:44:48.587723Z",
          "iopub.execute_input": "2024-12-19T17:44:48.588037Z",
          "iopub.status.idle": "2024-12-19T17:45:34.029332Z",
          "shell.execute_reply.started": "2024-12-19T17:44:48.58801Z",
          "shell.execute_reply": "2024-12-19T17:45:34.028396Z"
        },
        "id": "cIwELLa4ijOI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query_pipeline = transformers.pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        max_length = 10000,\n",
        ")"
      ],
      "metadata": {
        "_uuid": "7c62edb9-b191-401a-ae00-d3a7aa0637a5",
        "_cell_guid": "8402ad82-e4cb-4d75-9ac1-2095f1c475d1",
        "trusted": true,
        "_kg_hide-input": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "rEw3eecyijOL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json('/kaggle/input/casml-generative-ai-hackathon/Dataset_RAG (1)/queries.json')\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "_uuid": "65a684f5-8086-4b98-9aa8-cb3e89d69dd8",
        "_cell_guid": "53121b7d-b6c2-423d-b106-fb7fe257727d",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "j29hxJDxijOM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline=query_pipeline)"
      ],
      "metadata": {
        "_uuid": "b7308196-1214-4e82-9dec-a0d929ee3ba5",
        "_cell_guid": "166e228e-b594-4bfb-bec4-0071a7e9ceb4",
        "trusted": true,
        "_kg_hide-input": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "DC1DJ4gGijON"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "import pdfplumber\n",
        "\n",
        "def load_book_with_page_overlap(pdf_path, overlap_size=0):\n",
        "    \"\"\"Load and preprocess the book from PDF with word overlap between pages.\"\"\"\n",
        "    sections = []\n",
        "    # Function to add overlap between two pages\n",
        "    def overlap_pages(page_text, next_page_text, overlap_size):\n",
        "        page_words = page_text.split()\n",
        "        next_page_words = next_page_text.split()\n",
        "        # Take the last `overlap_size` words from the current page\n",
        "        overlap = page_words[-overlap_size:] if len(page_words) > overlap_size else page_words\n",
        "        combined_text = \" \".join(overlap) + \" \" + \" \".join(next_page_words)\n",
        "        return combined_text\n",
        "    try:\n",
        "        # Try loading with PyPDF2\n",
        "        reader = PdfReader(pdf_path, strict=False)  # Ignore structural issues\n",
        "        previous_page_text = None  # To hold the last page's text for overlap\n",
        "        for page_num, page in enumerate(reader.pages):\n",
        "            text = page.extract_text()\n",
        "            if text and text.strip():\n",
        "                if previous_page_text:\n",
        "                # Overlap the current page with the previous one\n",
        "                    combined_text = overlap_pages(previous_page_text, text, overlap_size)\n",
        "                    sections.append({\"page\": page_num + 1, \"text\": combined_text.strip()})\n",
        "                else:\n",
        "                    sections.append({\"page\": page_num + 1, \"text\": text.strip()})\n",
        "                # Update `previous_page_text` to be the current page's text for the next iteration\n",
        "                previous_page_text = text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading PDF with PyPDF2: {e}\")\n",
        "        print(\"Falling back to pdfplumber...\")\n",
        "        try:\n",
        "            # Use pdfplumber as a fallback\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                previous_page_text = None\n",
        "                for page_num, page in enumerate(pdf.pages):\n",
        "                    text = page.extract_text()\n",
        "                    if text and text.strip():\n",
        "                        if previous_page_text:\n",
        "                            # Overlap the current page with the previous one\n",
        "                            combined_text = overlap_pages(previous_page_text, text, overlap_size)\n",
        "                            sections.append({\"page\": page_num + 1, \"text\": combined_text.strip()})\n",
        "                        else:\n",
        "                            sections.append({\"page\": page_num + 1, \"text\": text.strip()})\n",
        "                        # Update `previous_page_text` for the next iteration\n",
        "                        previous_page_text = text.strip()\n",
        "        except Exception as fallback_error:\n",
        "            print(f\"Error loading PDF with pdfplumber: {fallback_error}\")\n",
        "            return None\n",
        "    return sections\n",
        "\n",
        "pdf_path = '/kaggle/input/casml-generative-ai-hackathon/Dataset_RAG (1)/book.pdf'\n",
        "book_data= load_book_with_page_overlap(pdf_path, overlap_size=5)\n",
        "\n",
        "if book_data:\n",
        "    print(f\"Loaded {len(book_data)} pages successfully.\")\n",
        "else:\n",
        "    print(\"Failed to load the PDF.\")"
      ],
      "metadata": {
        "_uuid": "1836b0df-e605-472d-b15b-9275e5a89194",
        "_cell_guid": "ecff2561-adea-4977-bd47-c50f0fc5c801",
        "trusted": true,
        "_kg_hide-input": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "2vhwM6Q0ijOO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i, section in enumerate(book_data[:5]):  # Show first 5 sections\n",
        "\n",
        "    print(f\"Page {section['page']} - Text: {section['text'][:150]}...\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Nd945za9ijOP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "dataset = Dataset.from_list(book_data)\n",
        "print(dataset)"
      ],
      "metadata": {
        "trusted": true,
        "id": "IIBuENlzijOP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "local_model_path = \"/kaggle/input/sentence-transformers/minilm-l6-v2/all-MiniLM-L6-v2\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=local_model_path, model_kwargs=model_kwargs)"
      ],
      "metadata": {
        "_uuid": "7ea4ca3f-3c71-480f-b795-76bc520064ae",
        "_cell_guid": "a94ff63f-de94-4d83-8577-b39af976efbd",
        "trusted": true,
        "_kg_hide-input": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-12-20T12:34:22.056463Z",
          "iopub.execute_input": "2024-12-20T12:34:22.057099Z",
          "iopub.status.idle": "2024-12-20T12:34:22.382906Z",
          "shell.execute_reply.started": "2024-12-20T12:34:22.05706Z",
          "shell.execute_reply": "2024-12-20T12:34:22.381444Z"
        },
        "id": "0YrBUCAOijOP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=section['text'],\n",
        "        metadata={\"page\": section['page']},\n",
        "    )\n",
        "    for section in book_data\n",
        "]\n",
        "\n",
        "# Initialize the text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "\n",
        "# Split the documents into smaller chunks\n",
        "all_splits = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "_uuid": "59ca8c10-cb39-4a9e-ba9e-d3f0951eec72",
        "_cell_guid": "72fb2c52-4b32-48af-a049-a99be047daa3",
        "trusted": true,
        "_kg_hide-input": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "2LMiZ_sXijOQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")"
      ],
      "metadata": {
        "_uuid": "a27e2821-8900-406e-b739-01ab60d287fc",
        "_cell_guid": "8c0576ba-dcd5-462f-94fc-b03f9d069639",
        "trusted": true,
        "_kg_hide-input": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "-9p1GpPnijOQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever()\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "_uuid": "0f253d63-f4fc-48d7-a180-828c62931993",
        "_cell_guid": "0c5db20a-adda-490f-a3b8-022e6bb88756",
        "trusted": true,
        "_kg_hide-input": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "LuPVoblnijOR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "def query_and_retrieve(query,query_id, vectordb):\n",
        "    \"\"\"\n",
        "    Perform a precision-based retrieval and generate a structured answer.\n",
        "\n",
        "    Args:\n",
        "        query (str): The query string.\n",
        "        vectordb: The vector database object to perform similarity searches.\n",
        "\n",
        "    Returns:\n",
        "        dict: The final structured answer and references.\n",
        "    \"\"\"\n",
        "    # Stage 1: Core concept retrieval\n",
        "    concept_results = vectordb.similarity_search(\n",
        "        query=f\"definition explanation {query}\",\n",
        "        k=1000   # Number of results to retrieve\n",
        "    )\n",
        "\n",
        "    # Stage 2: Supporting detail retrieval\n",
        "    detail_results = vectordb.similarity_search(\n",
        "        query=f\"example application {query}\",\n",
        "        k=1000\n",
        "    )\n",
        "\n",
        "    # Combine and prioritize contexts\n",
        "    contexts = []\n",
        "    pages = set()\n",
        "\n",
        "    # Process concept_results and detail_results\n",
        "    for results in [concept_results, detail_results]:\n",
        "        for doc in results:\n",
        "            contexts.append(doc.page_content)\n",
        "            pages.add(str(doc.metadata.get('page', 'Unknown')))\n",
        "\n",
        "    # Combine contexts and prepare the input for the QA system\n",
        "    combined_context = ' '.join(contexts[:2])  # Limit to top 2 relevant contexts\n",
        "    sources = list(sorted(pages))[:3]  # Limit references to top 3 pages\n",
        "\n",
        "    # Generate structured answer using the combined context\n",
        "    prompt = f\"\"\" refer context and answer\n",
        "\n",
        "\n",
        "### Input:\n",
        "**Context:**\n",
        "{combined_context}\n",
        "\n",
        "**Question:**\n",
        "{query}\n",
        "\n",
        "### Output:\n",
        "**Answer:**\n",
        "\n",
        "\"\"\"\n",
        "    # Call the QA system to generate the answer\n",
        "    try:\n",
        "        result = qa.run(prompt)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during model execution: {e}\")\n",
        "        result = \"Error generating answer\"\n",
        "\n",
        "    # Map the query to a precise section\n",
        "    section = map_to_precise_section(query, combined_context)\n",
        "\n",
        "    # Format the response\n",
        "    return {\n",
        "        'ID': query_id,\n",
        "        'context': combined_context,\n",
        "        'answer': result,\n",
        "        'references': json.dumps({\n",
        "            'sections': [section],\n",
        "            'pages': sources\n",
        "        })\n",
        "    }\n",
        "\n",
        "\n",
        "def create_submission_file(queries_df, vector_store):\n",
        "    \"\"\"\n",
        "    Create the submission file based on the provided queries and vector store.\n",
        "\n",
        "    Args:\n",
        "        queries_df (pd.DataFrame): DataFrame containing the test set queries and IDs.\n",
        "        vector_store: The vector database for document retrieval.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for _, row in queries_df.iterrows():\n",
        "        query_id = row['query_id']\n",
        "        query = row['question']\n",
        "\n",
        "        # Generate structured answers for each query\n",
        "        result = query_and_retrieve(query,query_id, vector_store)\n",
        "        results.append(result)\n",
        "\n",
        "    # Convert results to a DataFrame for submission\n",
        "    submission_df = pd.DataFrame(results)\n",
        "    submission_df.to_csv('submission.csv', index=False)\n",
        "    print(\"Submission file saved as 'submission.csv'\")\n",
        "\n",
        "def map_to_precise_section(query, context):\n",
        "    \"\"\"\n",
        "    Map the query to the most relevant section based on context and query keywords.\n",
        "    \"\"\"\n",
        "    section_mapping = {\n",
        "        'scientific': 'psychological_research/approaches_to_research',\n",
        "        'brain': 'biopsychology/the_brain_and_behavior',\n",
        "        'memory': 'memory/how_memory_functions',\n",
        "        'development': 'developmental_psychology/introduction',\n",
        "        'personality': 'personality/introduction_to_personality',\n",
        "        'emotion': 'emotion_and_motivation/emotions',\n",
        "        'learning': 'learning/introduction',\n",
        "        'social': 'social_psychology/introduction'\n",
        "    }\n",
        "\n",
        "    for key, section in section_mapping.items():\n",
        "        if key in query.lower() or key in context.lower():\n",
        "            return section\n",
        "    return \"introduction_to_psychology\"\n"
      ],
      "metadata": {
        "_uuid": "4ea5ff02-b2f9-4c1f-b0f2-e0d55f82b2ac",
        "_cell_guid": "5e4e1de3-1d17-4e16-b6de-9f72a98cb10c",
        "trusted": true,
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "_MJOPMBPijOR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "create_submission_file(df, vectordb)"
      ],
      "metadata": {
        "trusted": true,
        "id": "NaVdjggdijOT"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}